{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOVijZRSu1D1FKHGMbz4V8W"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"id":"evgBbeiGbgbv","executionInfo":{"status":"ok","timestamp":1702542366836,"user_tz":-60,"elapsed":6770,"user":{"displayName":"Keith Lowton","userId":"06128474876990122760"}},"outputId":"2c1c8111-5b45-4319-c40c-ba3abc14de93"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-990c282d-e3b7-4ef8-aead-900ba0d57669\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-990c282d-e3b7-4ef8-aead-900ba0d57669\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving kaggle.json to kaggle.json\n"]},{"output_type":"execute_result","data":{"text/plain":["{'kaggle.json': b'{\"username\":\"keithlowton\",\"key\":\"f96b94ee8178d07d81a253c5457d447a\"}'}"]},"metadata":{},"execution_count":1}],"source":["from google.colab import files\n","files.upload()"]},{"cell_type":"code","source":["from tensorflow import keras\n","model = keras.models.load_model(\"convnet_from_scratch_with_augmentation.ckpt\")\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"VCEbNCVIbp7K","executionInfo":{"status":"error","timestamp":1702544276288,"user_tz":-60,"elapsed":472,"user":{"displayName":"Keith Lowton","userId":"06128474876990122760"}},"outputId":"774412f3-9560-4e31-f9c8-ec46def9ed6f"},"execution_count":7,"outputs":[{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-7031b6287051>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"convnet_from_scratch_with_augmentation.ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;31m# Legacy case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m     return legacy_sm_saving_lib.load_model(\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/legacy/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                             raise IOError(\n\u001b[0m\u001b[1;32m    235\u001b[0m                                 \u001b[0;34mf\"No file or directory found at {filepath_str}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                             )\n","\u001b[0;31mOSError\u001b[0m: No file or directory found at convnet_from_scratch_with_augmentation.ckpt"]}]},{"cell_type":"code","source":["from tensorflow import keras\n","import numpy as np\n","\n","img_path = keras.utils.get_file(\n","    fname=\"cat.jpg\",\n","    origin=\"https://img-datasets.s3.amazonaws.com/cat.jpg\")\n","\n","def get_img_array(img_path, target_size):\n","    img = keras.utils.load_img(\n","        img_path, target_size=target_size)\n","    array = keras.utils.img_to_array(img)\n","    array = np.expand_dims(array, axis=0)\n","    return array\n","\n","img_tensor = get_img_array(img_path, target_size=(180, 180))"],"metadata":{"id":"SfvCFfHmbt0D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","plt.axis(\"off\")\n","plt.imshow(img_tensor[0].astype(\"uint8\"))\n","plt.show()"],"metadata":{"id":"yQRfN2ZYb1D3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Instantiating a model that returns layer activations**"],"metadata":{"id":"GYVmx2f4cA8q"}},{"cell_type":"code","source":["from tensorflow.keras import layers\n","\n","layer_outputs = []\n","layer_names = []\n","for layer in model.layers:\n","    if isinstance(layer, (layers.Conv2D, layers.MaxPooling2D)):\n","        layer_outputs.append(layer.output)\n","        layer_names.append(layer.name)\n","activation_model = keras.Model(inputs=model.input, outputs=layer_outputs)"],"metadata":{"id":"_1Lxsw8Eb3TO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Using the model to compute layer activations"],"metadata":{"id":"i2zAQeTsb90y"}},{"cell_type":"code","source":["activations = activation_model.predict(img_tensor)"],"metadata":{"id":"TAKJAY27b5df"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["first_layer_activation = activations[0]\n","print(first_layer_activation.shape)"],"metadata":{"id":"ELTZOjiWcFQt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Visualizing the fifth channel**"],"metadata":{"id":"AoyxzpSLcJ8p"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","plt.matshow(first_layer_activation[0, :, :, 5], cmap=\"viridis\")"],"metadata":{"id":"Nmm2F2nccHf-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Visualizing every channel in every intermediate activation**"],"metadata":{"id":"h6LQG9UlcQCo"}},{"cell_type":"code","source":["images_per_row = 16\n","for layer_name, layer_activation in zip(layer_names, activations):\n","    n_features = layer_activation.shape[-1]\n","    size = layer_activation.shape[1]\n","    n_cols = n_features // images_per_row\n","    display_grid = np.zeros(((size + 1) * n_cols - 1,\n","                             images_per_row * (size + 1) - 1))\n","    for col in range(n_cols):\n","        for row in range(images_per_row):\n","            channel_index = col * images_per_row + row\n","            channel_image = layer_activation[0, :, :, channel_index].copy()\n","            if channel_image.sum() != 0:\n","                channel_image -= channel_image.mean()\n","                channel_image /= channel_image.std()\n","                channel_image *= 64\n","                channel_image += 128\n","            channel_image = np.clip(channel_image, 0, 255).astype(\"uint8\")\n","            display_grid[\n","                col * (size + 1): (col + 1) * size + col,\n","                row * (size + 1) : (row + 1) * size + row] = channel_image\n","    scale = 1. / size\n","    plt.figure(figsize=(scale * display_grid.shape[1],\n","                        scale * display_grid.shape[0]))\n","    plt.title(layer_name)\n","    plt.grid(False)\n","    plt.axis(\"off\")\n","    plt.imshow(display_grid, aspect=\"auto\", cmap=\"viridis\")"],"metadata":{"id":"ri7IZXaacN6f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Visualizing convnet filters  \n","\n","**Instantiating the Xception convolutional base**"],"metadata":{"id":"MEwWQo_4cXRf"}},{"cell_type":"code","source":["model = keras.applications.xception.Xception(\n","    weights=\"imagenet\",\n","    include_top=False)"],"metadata":{"id":"os32Ya1rcWwk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for layer in model.layers:\n","    if isinstance(layer, (keras.layers.Conv2D, keras.layers.SeparableConv2D)):\n","        print(layer.name)"],"metadata":{"id":"RdqhciC0cgAv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Creating a feature extractor model**"],"metadata":{"id":"Z0oozy-7ckhQ"}},{"cell_type":"code","source":["layer_name = \"block3_sepconv1\"\n","layer = model.get_layer(name=layer_name)\n","feature_extractor = keras.Model(inputs=model.input, outputs=layer.output)"],"metadata":{"id":"CFdmWisIcieX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["activation = feature_extractor(\n","    keras.applications.xception.preprocess_input(img_tensor)\n",")"],"metadata":{"id":"DmCHy810cnd6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","\n","def compute_loss(image, filter_index):\n","    activation = feature_extractor(image)\n","    filter_activation = activation[:, 2:-2, 2:-2, filter_index]\n","    return tf.reduce_mean(filter_activation)"],"metadata":{"id":"GJN9XlEYcn-6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Loss maximization via stochastic gradient ascent**"],"metadata":{"id":"W1uZl-ogcvkP"}},{"cell_type":"code","source":["@tf.function\n","def gradient_ascent_step(image, filter_index, learning_rate):\n","    with tf.GradientTape() as tape:\n","        tape.watch(image)\n","        loss = compute_loss(image, filter_index)\n","    grads = tape.gradient(loss, image)\n","    grads = tf.math.l2_normalize(grads)\n","    image += learning_rate * grads\n","    return image"],"metadata":{"id":"K4izpmEzctN9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Function to generate filter visualizations**"],"metadata":{"id":"j3Tc_DVwc0PY"}},{"cell_type":"code","source":["img_width = 200\n","img_height = 200\n","\n","def generate_filter_pattern(filter_index):\n","    iterations = 30\n","    learning_rate = 10.\n","    image = tf.random.uniform(\n","        minval=0.4,\n","        maxval=0.6,\n","        shape=(1, img_width, img_height, 3))\n","    for i in range(iterations):\n","        image = gradient_ascent_step(image, filter_index, learning_rate)\n","    return image[0].numpy()"],"metadata":{"id":"7miJaMINcyYg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Utility function to convert a tensor into a valid image**"],"metadata":{"id":"EKoR-HyHc3ny"}},{"cell_type":"code","source":["def deprocess_image(image):\n","    image -= image.mean()\n","    image /= image.std()\n","    image *= 64\n","    image += 128\n","    image = np.clip(image, 0, 255).astype(\"uint8\")\n","    image = image[25:-25, 25:-25, :]\n","    return image"],"metadata":{"id":"XEENB1KZc3Ab"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.axis(\"off\")\n","plt.imshow(deprocess_image(generate_filter_pattern(filter_index=2)))"],"metadata":{"id":"CuqWhB0lc8J8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Generating a grid of all filter response patterns in a layer**"],"metadata":{"id":"wDdVd5LIdAVG"}},{"cell_type":"code","source":["all_images = []\n","for filter_index in range(64):\n","    print(f\"Processing filter {filter_index}\")\n","    image = deprocess_image(\n","        generate_filter_pattern(filter_index)\n","    )\n","    all_images.append(image)\n","\n","margin = 5\n","n = 8\n","cropped_width = img_width - 25 * 2\n","cropped_height = img_height - 25 * 2\n","width = n * cropped_width + (n - 1) * margin\n","height = n * cropped_height + (n - 1) * margin\n","stitched_filters = np.zeros((width, height, 3))\n","\n","for i in range(n):\n","    for j in range(n):\n","        image = all_images[i * n + j]\n","        stitched_filters[\n","            (cropped_width + margin) * i : (cropped_width + margin) * i + cropped_width,\n","            (cropped_height + margin) * j : (cropped_height + margin) * j\n","            + cropped_height,\n","            :,\n","        ] = image\n","\n","keras.utils.save_img(\n","    f\"filters_for_layer_{layer_name}.png\", stitched_filters)"],"metadata":{"id":"HaFMZTtTc-UP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Visualizing heatmaps of class activation\n","\n","**Loading the Xception network with pretrained weights**"],"metadata":{"id":"S-XISCHQdG27"}},{"cell_type":"code","source":["model = keras.applications.xception.Xception(weights=\"imagenet\")"],"metadata":{"id":"ywXh0M3PdEjK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Preprocessing an input image for Xception**"],"metadata":{"id":"YVmb1b-YdQB-"}},{"cell_type":"code","source":["img_path = keras.utils.get_file(\n","    fname=\"elephant.jpg\",\n","    origin=\"https://img-datasets.s3.amazonaws.com/elephant.jpg\")\n","\n","def get_img_array(img_path, target_size):\n","    img = keras.utils.load_img(img_path, target_size=target_size)\n","    array = keras.utils.img_to_array(img)\n","    array = np.expand_dims(array, axis=0)\n","    array = keras.applications.xception.preprocess_input(array)\n","    return array\n","\n","img_array = get_img_array(img_path, target_size=(299, 299))"],"metadata":{"id":"UbPFbc4VdNZi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["preds = model.predict(img_array)\n","print(keras.applications.xception.decode_predictions(preds, top=3)[0])"],"metadata":{"id":"vwq5jFLMdS2h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.argmax(preds[0])"],"metadata":{"id":"Te7HrFKYdUra"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Setting up a model that returns the last convolutional output**"],"metadata":{"id":"fkEYhPOsdY6t"}},{"cell_type":"code","source":["last_conv_layer_name = \"block14_sepconv2_act\"\n","classifier_layer_names = [\n","    \"avg_pool\",\n","    \"predictions\",\n","]\n","last_conv_layer = model.get_layer(last_conv_layer_name)\n","last_conv_layer_model = keras.Model(model.inputs, last_conv_layer.output)"],"metadata":{"id":"RdnVdi8gdYC0"},"execution_count":null,"outputs":[]}]}